{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation of Recommendandation system for a dummy data\n",
        "\n",
        "Evaluation parameters:\n",
        "\n",
        "1.  Mean square error\n",
        "2. Mean Absolute Error\n",
        "3. Precision@k\n",
        "4. Recall@k\n",
        "5. Mean Average Precision\n",
        "6. Mean Reciprocal Rank"
      ],
      "metadata": {
        "id": "Nn7tw2zSbOew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Generate dummy data\n",
        "np.random.seed(0)\n",
        "\n",
        "# Number of users and items\n",
        "num_users = 10\n",
        "num_items = 5\n",
        "\n",
        "# Generate a random rating matrix (user-item matrix) where 0 means no rating\n",
        "ratings = np.random.randint(0, 6, size=(num_users, num_items))  # Ratings between 0 to 5\n",
        "ratings[ratings < 3] = 0  # Set ratings less than 3 to 0 to simulate missing ratings\n",
        "\n",
        "# Generate a dummy prediction matrix for the purpose of evaluation\n",
        "predictions = np.random.uniform(1, 5, size=(num_users, num_items))\n",
        "\n",
        "# Flatten the matrices for evaluation\n",
        "# Mask out entries with 0 rating (assuming they are unrated)\n",
        "rated_indices = ratings > 0\n",
        "true_ratings = ratings[rated_indices]\n",
        "pred_ratings = predictions[rated_indices]\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mse = mean_squared_error(true_ratings, pred_ratings)\n",
        "mae = mean_absolute_error(true_ratings, pred_ratings)\n",
        "\n",
        "# Print results\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)"
      ],
      "metadata": {
        "id": "y42DICrmbWAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "362e06bd-7667-4d45-e62c-d5509e50c66a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 3.040845253294991\n",
            "Mean Absolute Error (MAE): 1.4648213766228029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "k = 3  # Define the cutoff rank (top-k)\n",
        "\n",
        "# Dummy data: ground truth relevant items for each user\n",
        "# Each user has a list of items they interacted with (considered relevant)\n",
        "ground_truth = {\n",
        "    0: [1, 3, 4],\n",
        "    1: [0, 2],\n",
        "    2: [1, 4],\n",
        "    3: [3],\n",
        "    4: [0, 2, 4],\n",
        "    5: [2, 3],\n",
        "    6: [1],\n",
        "    7: [0, 1, 2],\n",
        "    8: [4],\n",
        "    9: [1, 3]\n",
        "}\n",
        "\n",
        "# Dummy data: recommended items for each user\n",
        "# Each user has a list of recommended items (sorted by relevance)\n",
        "recommendations = {\n",
        "    0: [3, 1, 4, 2, 0],\n",
        "    1: [2, 0, 3, 4, 1],\n",
        "    2: [1, 0, 4, 3, 2],\n",
        "    3: [3, 2, 1, 0, 4],\n",
        "    4: [0, 4, 2, 3, 1],\n",
        "    5: [3, 2, 0, 1, 4],\n",
        "    6: [1, 4, 3, 2, 0],\n",
        "    7: [0, 1, 2, 3, 4],\n",
        "    8: [4, 3, 2, 1, 0],\n",
        "    9: [1, 3, 4, 0, 2]\n",
        "}\n",
        "\n",
        "# Precision@k and Recall@k\n",
        "def precision_at_k(recommended, relevant, k):\n",
        "    recommended_k = recommended[:k]\n",
        "    relevant_k = set(recommended_k) & set(relevant)\n",
        "    return len(relevant_k) / k\n",
        "\n",
        "def recall_at_k(recommended, relevant, k):\n",
        "    recommended_k = recommended[:k]\n",
        "    relevant_k = set(recommended_k) & set(relevant)\n",
        "    return len(relevant_k) / len(relevant) if relevant else 0\n",
        "\n",
        "# Mean Average Precision@k\n",
        "def average_precision_at_k(recommended, relevant, k):\n",
        "    relevant_k = set(relevant)\n",
        "    score = 0.0\n",
        "    num_hits = 0\n",
        "    for i, item in enumerate(recommended[:k]):\n",
        "        if item in relevant_k:\n",
        "            num_hits += 1\n",
        "            score += num_hits / (i + 1)\n",
        "    return score / min(len(relevant), k) if relevant else 0\n",
        "\n",
        "# Mean Reciprocal Rank@k\n",
        "def reciprocal_rank_at_k(recommended, relevant, k):\n",
        "    for i, item in enumerate(recommended[:k]):\n",
        "        if item in relevant:\n",
        "            return 1 / (i + 1)\n",
        "    return 0\n",
        "\n",
        "# Calculate metrics for all users and average them\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "map_scores = []\n",
        "mrr_scores = []\n",
        "\n",
        "for user in ground_truth.keys():\n",
        "    relevant_items = ground_truth[user]\n",
        "    recommended_items = recommendations[user]\n",
        "\n",
        "    precision_scores.append(precision_at_k(recommended_items, relevant_items, k))\n",
        "    recall_scores.append(recall_at_k(recommended_items, relevant_items, k))\n",
        "    map_scores.append(average_precision_at_k(recommended_items, relevant_items, k))\n",
        "    mrr_scores.append(reciprocal_rank_at_k(recommended_items, relevant_items, k))\n",
        "\n",
        "# Averaging the scores\n",
        "mean_precision = np.mean(precision_scores)\n",
        "mean_recall = np.mean(recall_scores)\n",
        "mean_map = np.mean(map_scores)\n",
        "mean_mrr = np.mean(mrr_scores)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Precision@{k}: {mean_precision:.4f}\")\n",
        "print(f\"Recall@{k}: {mean_recall:.4f}\")\n",
        "print(f\"MAP@{k}: {mean_map:.4f}\")\n",
        "print(f\"MRR@{k}: {mean_mrr:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOw3OK40bBOn",
        "outputId": "cb2aef54-3328-4eea-f989-7e6751257ee4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision@3: 0.6667\n",
            "Recall@3: 1.0000\n",
            "MAP@3: 0.9833\n",
            "MRR@3: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task to do: Build a movie recommendation system and evaluate it by the above defined parameters."
      ],
      "metadata": {
        "id": "0YJ2RIb_bNg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aVOQb-sCXzs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "16dASoChdGZs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/ratings_movies.csv'\n",
        "data = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "Z75QatjLX-UK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a user-item matrix\n",
        "user_item_matrix = data.pivot(index='userId', columns='movieId', values='rating').fillna(0)"
      ],
      "metadata": {
        "id": "zKt15drLYAII"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute cosine similarity between users\n",
        "user_similarity = cosine_similarity(user_item_matrix)\n",
        "np.fill_diagonal(user_similarity, 0)  # Set diagonal to 0 to ignore self-similarity\n"
      ],
      "metadata": {
        "id": "9iyTKimJYBi-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions by averaging ratings of similar users (weighted by similarity)\n",
        "predictions = user_similarity.dot(user_item_matrix) / np.array([np.abs(user_similarity).sum(axis=1)]).T\n",
        "\n",
        "# Mask actual ratings (to simulate the test scenario) and evaluate\n",
        "rated_indices = user_item_matrix > 0  # Indices of actual ratings\n",
        "true_ratings = user_item_matrix.values[rated_indices]\n",
        "pred_ratings = predictions[rated_indices]"
      ],
      "metadata": {
        "id": "4hh5JNzJZUfD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate MSE and MAE\n",
        "mse = mean_squared_error(true_ratings, pred_ratings)\n",
        "mae = mean_absolute_error(true_ratings, pred_ratings)\n"
      ],
      "metadata": {
        "id": "0XR3kqCrZWOy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLTwhOu5ZX1p",
        "outputId": "32dfe0b3-9c9e-4058-e8ab-88c3e491d919"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 9.72605155620528\n",
            "Mean Absolute Error (MAE): 2.938697599228125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters for Precision@k, Recall@k, MAP@k, MRR@k\n",
        "k = 7\n"
      ],
      "metadata": {
        "id": "RX2EWA6SZZpk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate top-k recommendations for each user\n",
        "top_k_recommendations = np.argsort(-predictions, axis=1)[:, :k]"
      ],
      "metadata": {
        "id": "eBqBT1q9Zdbk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(top_k_recommendations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8tVQ3d5ZfWx",
        "outputId": "99ef891f-95d5-4cc4-873e-4e1dfca7229d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 257  314  277 ...  224  510  897]\n",
            " [ 277 1938  314 ... 6693  257 7355]\n",
            " [ 224  897 1938 ...  510  910  257]\n",
            " ...\n",
            " [ 314  277  257 ...  510  224 2224]\n",
            " [ 314  277  257 ...  510  123  418]\n",
            " [ 277 1938  314 ...  224 2224  510]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ground truth relevant items (non-zero actual ratings)\n",
        "ground_truth = {i: np.where(user_item_matrix.iloc[i] > 0)[0].tolist() for i in range(user_item_matrix.shape[0])}"
      ],
      "metadata": {
        "id": "Tms5P7YBZi5o"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_at_k(recommended, relevant, k):\n",
        "    recommended_k = recommended[:k]\n",
        "    relevant_k = set(recommended_k) & set(relevant)\n",
        "    return len(relevant_k) / k"
      ],
      "metadata": {
        "id": "JA63EkwrZlod"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recall_at_k(recommended, relevant, k):\n",
        "    recommended_k = recommended[:k]\n",
        "    relevant_k = set(recommended_k) & set(relevant)\n",
        "    return len(relevant_k) / len(relevant) if relevant else 0"
      ],
      "metadata": {
        "id": "nzYno9pHZn1p"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def average_precision_at_k(recommended, relevant, k):\n",
        "    relevant_k = set(relevant)\n",
        "    score = 0.0\n",
        "    num_hits = 0\n",
        "    for i, item in enumerate(recommended[:k]):\n",
        "        if item in relevant_k:\n",
        "            num_hits += 1\n",
        "            score += num_hits / (i + 1)\n",
        "    return score / min(len(relevant), k) if relevant else 0"
      ],
      "metadata": {
        "id": "Fr9WvRnVZpvT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reciprocal_rank_at_k(recommended, relevant, k):\n",
        "    for i, item in enumerate(recommended[:k]):\n",
        "        if item in relevant:\n",
        "            return 1 / (i + 1)\n",
        "    return 0"
      ],
      "metadata": {
        "id": "kJkYHpZEZraK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate metrics for all users\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "map_scores = []\n",
        "mrr_scores = []\n",
        "\n",
        "for user in range(user_item_matrix.shape[0]):\n",
        "    relevant_items = ground_truth[user]\n",
        "    recommended_items = top_k_recommendations[user]\n",
        "\n",
        "    precision_scores.append(precision_at_k(recommended_items, relevant_items, k))\n",
        "    recall_scores.append(recall_at_k(recommended_items, relevant_items, k))\n",
        "    map_scores.append(average_precision_at_k(recommended_items, relevant_items, k))\n",
        "    mrr_scores.append(reciprocal_rank_at_k(recommended_items, relevant_items, k))"
      ],
      "metadata": {
        "id": "prPzSJ3ZZsv7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate averages\n",
        "mean_precision = np.mean(precision_scores)\n",
        "mean_recall = np.mean(recall_scores)\n",
        "mean_map = np.mean(map_scores)\n",
        "mean_mrr = np.mean(mrr_scores)\n",
        "\n",
        "print(f\"Precision@{k}: {mean_precision:.4f}\")\n",
        "print(f\"Recall@{k}: {mean_recall:.4f}\")\n",
        "print(f\"MAP@{k}: {mean_map:.4f}\")\n",
        "print(f\"MRR@{k}: {mean_mrr:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAAygN_mZusf",
        "outputId": "03202a41-41cd-46b1-8dcc-68003fb8f84c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision@7: 0.5974\n",
            "Recall@7: 0.0603\n",
            "MAP@7: 0.5135\n",
            "MRR@7: 0.8366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d7IOFDpDZwcQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}